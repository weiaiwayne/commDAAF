# 2026-02-27 Session Notes

## Virality Study Progress - MAJOR UPDATE

### Dataset Split Decision
Split 719-post dataset into two separate studies:
- **#MahsaAmini**: 400 posts (viral_mahsa_full.jsonl)
- **Ukraine**: 319 posts (viral_ukraine_full.jsonl)

Rationale: Prevent methodological confusion from mixing fundamentally different crisis contexts.

### Current Coding Status (as of ~22:00 UTC)

| Model | #MahsaAmini | Ukraine | Notes |
|-------|-------------|---------|-------|
| Claude | 400/400 âœ… | 319/319 âœ… | Full CommDAAF methodology |
| Kimi | 400/400 ðŸ”„ | Pending | CommDAAF re-run in progress |
| GLM | 200/400 â³ | Pending | Batches 1-2 done, 3-4 blocked |

### Kimi CommDAAF Re-Run (Major Decision)
**Problem discovered:** Original Kimi coding used simplified prompts without CommDAAF decision rules, valence anchors, arousal anchors, or Persian handling guidance.

**Solution:** Created full CommDAAF coding prompt (`commdaaf_coding_prompt.md`) and re-running all 400 #MahsaAmini posts through Kimi with proper methodology.

**Implementation:**
- Split 400 posts into 16 sub-batches of 25 posts each (Kimi truncates JSON on larger batches)
- Batch 4 (sub-batches a,b,c,d): âœ… Complete & verified
- Batches 1-3 (12 sub-batches): Launched via OpenCode, most completing

**Sub-batch sessions (Kimi CommDAAF):**
- Batch 1: warm-atlas, mild-breeze, wild-lobster, nova-shoal
- Batch 2: tide-wharf, wild-shoal, brisk-daisy, calm-coral
- Batch 3: wild-mist, tidy-trail, amber-nudibranch, brisk-pine

### Files Created/Updated Today
- `commdaaf_coding_prompt.md` - Full CommDAAF methodology prompt
- `claude_mahsa_coding.json` - 400 posts
- `claude_ukraine_coding.json` - 319 posts
- `kimi_batch4{a,b,c,d}_commdaaf.json` - 25 posts each, verified
- `viral_mahsa_batch{1-4}{a,b,c,d}.jsonl` - 25-post sub-batches

### GLM Status
- Batches 1-2: Complete via `zai/glm-4.7` (pay-per-token API)
- Batches 3-4: **Blocked** - awaiting decision on approach
- Options: (a) smaller batches via OpenCode, (b) skip for this study

### Completed Today
1. âœ… All 3 models re-coded with full CommDAAF (Claude, GLM, Kimi)
2. âœ… Fixed data contamination (removed 20 Ukraine posts from mahsa dataset)
3. âœ… 3-model reliability: Fleiss' Îº = 0.633 (Substantial, up from 0.574)
4. âœ… Corrected regression: Negative Binomial (not OLS)
5. âœ… Key finding: INFORMATIONAL (IRR=2.72) > CONFLICT for engagement
6. âœ… Added regression modeling guidance to CommDAAF
7. âœ… All pushed to GitHub

### Ukraine Study - RESUMED
- Initially skipped due to noise concerns, but resumed with sub-batch approach
- **Batch structure**: 14 sub-batches (batches 1-4, each split into a/b/c/d, 25 posts each)
- **Claude progress**: Batches 1a, 1b complete â†’ `claude_ukraine_1a_commdaaf.json`, `claude_ukraine_1b_commdaaf.json`
- **Remaining**: 12 sub-batches (1c, 1d, 2a-d, 3a-d, 4a-b)
- **Next**: Complete Claude, then run GLM + Kimi on same batches

### #MahsaAmini Final Results (n=380)
- **INFORMATIONAL** IRR=2.72 (p<.001) - TOP predictor
- **High arousal** IRR=1.58 (p=.038)
- **Neutral valence** IRR=0.51 (p=.005) - reduces engagement
- **Fleiss' Îº** = 0.633 (Substantial agreement across 3 models)

## Technical Notes

### Kimi Batch Size Limitation
Kimi truncates JSON output on batches >25-30 posts. Solution: Split into 25-post sub-batches.

### OpenCode PTY Requirement
Coding agents REQUIRE `pty:true` in exec calls. Without PTY:
- Output breaks or hangs
- Commands appear to run but produce no output

### API Configuration (for AgentAcademy)
- **Kimi**: Use `kimi-coding/k2p5` via OpenCode (flat-rate coding plan)
- **GLM**: Use `zai-coding-plan/glm-4.7` via OpenCode (flat-rate) - but stalls on large tasks
- **DO NOT** use Mei agent/OpenRouter for AgentAcademy (wrong billing model)

### CommDAAF v0.5 Updates Applied
- Frame decision rules with hierarchical priority
- Valence anchors (Persian examples)
- Arousal anchors
- Mixed-language handling (Persian + Arabic + English)
- Single vs multi-model QC guidelines
