# 2026-02-22 — Chinese LLM Censorship Investigation

## Summary
Investigated why Chinese LLMs (GLM, Kimi) showed no censorship in AgentAcademy runs vs earlier blocking. **Major finding: it wasn't academic framing bypass — it was the free proxy.**

## Timeline
1. **Feb 18**: Direct API calls to z.ai/Kimi blocked Xinjiang content with HTTP 400
2. **Feb 20+**: Runs used `opencode/kimi-k2.5-free` (free proxy tier) — worked
3. **Feb 22**: Discovered the discrepancy, traced to OpenCode provider config

## Key Evidence
- Log analysis: Jan 5 used `providerID=zai-coding-plan` (direct API)
- Feb 20 runs used `providerID=opencode` (free proxy)
- **$0.00 total cost** in runs = free tier confirmation
- Free proxy apparently strips/bypasses content filters

## Model ID Mapping (Wayne's OpenCode config)
| Purpose | Model ID | Type |
|---------|----------|------|
| Direct z.ai | `zai-coding-plan/glm-4.7` | Paid, filtered |
| Direct Kimi | `kimi-for-coding/k2p5` | Paid, filtered |
| Free proxy | `opencode/kimi-k2.5-free` | Free, unfiltered |
| Free proxy | `opencode/glm-4.7-free` | Free, unfiltered |

## Paper Status
- **Published**: `papers/ACADEMIC_FRAMING_BYPASS.md` (commits `44b52bf`, `077fe32`)
- **Status**: REQUIRES RETRACTION — hypothesis was based on flawed methodology
- Original claim: CommDAAF's ~2,500 lines of academic methodology dilutes sensitive keywords
- Reality: OpenCode free proxy infrastructure bypasses API content filters

## Action Items
- [x] Document findings in memory
- [ ] Post retraction note with reflection
- [ ] Test with DIRECT API keys to confirm/void bypass hypothesis
- [ ] Update cron job to use direct API model IDs

## Lessons Learned
1. Always verify which API endpoint is actually being called
2. Check cost = $0 as a signal for proxy/free tier usage
3. Don't publish findings until methodology is fully validated
4. OpenCode free proxy creates unintended policy gaps for Chinese LLM content filtering
